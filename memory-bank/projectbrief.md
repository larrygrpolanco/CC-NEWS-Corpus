# Project Brief: Common Crawl News Corpus for Research

## 1. Project Goal
To create a curated corpus of news articles from the Common Crawl database. This corpus will be used for research purposes, likely focusing on analyzing linguistic features, persuasion techniques, and variations across different news outlets and time periods.

## 2. Core Requirements
- Access and organize data from Common Crawl.
- Identify and extract articles from specific news publishers (initially focusing on The New York Times).
- Develop scripts for data processing, including fetching relevant index data and WARC files.
- Address challenges such as publishers blocking crawlers (e.g., NYT potentially after 2023).

## 3. Scope
- Initial phase: Determine the latest Common Crawl data available for The New York Times.
- Subsequent phases:
    - Develop methods to extract article URLs for target publishers.
    - Download relevant WARC segments.
    - Parse articles (headline, author, date, body text).
    - Potentially expand to other publishers (e.g., Washington Post).
- The project involves understanding Common Crawl structure (CC-MAIN, CC-NEWS, CDX indexes, WARC files).
